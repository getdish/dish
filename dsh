#!/bin/bash

if [ "$KEEP_ALIVE" != "true" ]; then
  set -eo pipefail
fi

CWD_DIR=$(pwd)
DISH_ENV="${DISH_ENV:-production}"
ENV_FILE=".env.$DISH_ENV"
SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )"
export PROJECT_ROOT=$SCRIPT_DIR

cd "$PROJECT_ROOT"

function shell() {
  hostvar="$1_HOST"
  host="${!hostvar}"
  ssh-add -K etc/keys/server_rsa
  ssh "root@$host"
}

function deploy() {
  if [ "$1" = "all" ]; then
    deploy_all_to io1
  elif [ "$1" = "ci" ]; then
    deploy_ci_to io2
  else
    echo "no deploy command"
    exit 1
  fi
}

function exec() {
  app=${1:-app}
  shift
  cmd=$*
  echo "exec $app: running $cmd"
  docker exec -it $(docker ps | grep $app | head -n1 | awk '{print $1}') $cmd
}

function compose() {
  docker-compose -f docker-$1.yml $2 $3
}

function compose_build() {
  echo "🐳 compose build... $@ $TIMESCALE_IMAGE"
  if [ "$1" = "base" ] || [ "$1" = "run-tests" ]; then
    docker-compose -f docker-internal.yml build "$@"
  else
    docker-compose build "$@"
  fi
}

function compose_build_and_push() {
  compose_build "$@"
  echo "🐳 docker push... $@"
  docker push "registry.dishapp.com/dish-$@" || echo "not our image, skip"
}

function compose_build_and_push_all() {
  compose_build_and_push base
  services=$(docker-compose config --services 2>/dev/null | tr '\r\n' ' ')
  for service in $services; do
    compose_build_and_push $service
  done
  compose_build_and_push run-tests
}

function compose_up_subset() {
  services=$1
  extra=$2
  flags=""
  if [ "$DOCKER_NO_RECREATE" != "true" ]; then
    flags="--remove-orphans --force-recreate"
  fi
  echo "docker-compose up $flags $extra $services ($POSTGRES_DB $TIMESCALE_HOST $TIMESCALE_PORT_INTERNAL)"
  if [ -z "$extra" ]; then
    docker-compose up $flags $services
  else
    docker-compose up $flags "$extra" $services
  fi
  printf "\n\n\n"
}

function compose_up() {
  services_list="$COMPOSE_EXCLUDE${COMPOSE_EXCLUDE_EXTRA:-}"
  services=$(
    docker-compose config --services 2>/dev/null |
      grep -E -v "$services_list" |
      tr '\r\n' ' '
  )
  docker_login
  # cleans up misbehaving old containers
  if [ "$DISH_ENV" = "test" ]; then
    echo "in env test cleanup old services first"
    for service in $services; do
      docker-compose rm --force "$service" || true
    done
  fi
  compose_up_subset "$services" "$@"
}

function deploy_ci_to() {
  local hostvar="$1_HOST"
  local host="${!hostvar}"
  ssh -i "$PROJECT_ROOT/etc/keys/server_rsa" -o StrictHostKeyChecking=no "root@$host" "
    cd /app
    ./dsh deploy_ci
    echo done ✅
  "
}

function deploy_all_to() {
  echo "deploying to $1"
  sync_to "$1"
  deploy_to "$1"
  printf "\n\ndeploy done ✅\n\n"
}

function deploy_to() {
  hostvar="$1_HOST"
  host="${!hostvar}"
  key="$PROJECT_ROOT/etc/keys/server_rsa"
  chmod 600 "$key"
  if [ "$host" = "" ] || [ ! -f "$key" ]; then
    echo "no host or private key $host $key"
    exit 1
  fi
  echo " 🖥  deploy (swarm)..."
  ssh -i "$key" -o StrictHostKeyChecking=no "root@$host" "
    echo start deploy
    set -e
    cd /app
    echo setup env
    source .env
    source .env.production
    echo deploy
    ./dsh deploy_all
    docker system prune --force || true &
    echo done
  "
}

function docker_logs() {
  journalctl -u docker.service | tail -n 50
  docker events
}

function docker_prune() {
  docker system prune -f
}

function rsync_server_backup() {
  echo "backing up..."
  ionice -c2 -n7 \
    rsync -aAXv / --exclude={"/var/data/docker/*","/dev/*","/proc/*","/sys/*","/tmp/*","/run/*","/mnt/*","/media/*","/lost+found"} de1257@de1257.rsync.net:backup
}

function destroy_all() {
  docker stack rm traefik || true
  docker stack rm portainer || true
  docker stack rm registry || true
  docker stack rm swarmpron || true
  docker stack rm dish || true
  docker system prune -f
}

function deploy_all() {
  echo "deploying all services"
  deploy_traefik
  deploy_portainer
  deploy_registry
  sleep 15 # let registry come online, todo see wait_until
  docker_login || exit 1
  deploy_swarmprom
  deploy_dish
}

# shellcheck disable=SC2120
function deploy_dish() {
  echo "deploying dish - $DISH_ENV $POSTGRES_DB $POSTGRES_DB_DIR"
  docker stack deploy --with-registry-auth --prune -c docker-compose.yml dish ||
    echo "retry" && docker stack deploy --with-registry-auth --prune -c docker-compose.yml dish
  if [ "$1" == "--force" ]; then
    #  force restart
    docker stack services -q dish | while read -r service; do
      echo "dish restarting: $service"
      # date out of sequence errors on backgrounding &
      # docker_restart "$service" &
      docker_restart "$service"
    done
    wait
  fi
  # migrate_all
}

function docker_restart() {
  docker service update --force "$1"
}

function migrate_all() {
  wait_until_services_ready
  migrate_hasura
  migrate_timescale
  migrate_umami
}

function deploy_one() {
  # stack service
  docker service update --force $1_$2
}

function deploy_ci() {
  compose ci build
  compose ci down || true
  compose ci rm -f || true
  compose ci up -d
}

function deploy_registry() {
  compose registry build
  compose registry push
  docker stack deploy --prune -c docker-registry.yml registry
  docker service update --force registry_registry-proxy
  docker service update --force registry_registry
}

function deploy_portainer() {
  docker stack deploy --prune -c docker-portainer.yml portainer ||
    echo "retry" && docker stack deploy --prune -c docker-portainer.yml portainer
  docker service update --force portainer_portainer
  docker service ps --no-trunc portainer_portainer
}

function deploy_traefik() {
  docker network create --driver=overlay traefik-public &>/dev/null || true
  # sometimes fails with "update out of sequence" but thats ok, retry
  docker stack deploy --prune -c docker-traefik.yml traefik || 
    docker stack deploy --prune -c docker-traefik.yml traefik
  docker service update --force traefik_traefik
  docker service ps --no-trunc traefik_traefik
}

function docker_login() {
  docker login registry.dishapp.com -u "$TRAEFIK_USERNAME" -p "$TRAEFIK_PASSWORD_PLAIN" &> /dev/null
}

function deploy_internal() {
  docker-compose -f docker-internal.yml build ci
  docker-compose -f docker-internal.yml up -d ci
}

function deploy_swarmprom() {
  # ensure images needed next in registry
  build_swarmprom_images
  docker stack deploy --prune -c docker-swarmprom.yml swarmprom
}

function build_swarmprom_images() {
  compose swarmprom build
  compose swarmprom push
}

function log_ci() {
  docker logs app_ci_1 "$@"
}

function log_traefik() {
  docker service logs traefik_traefik "$@"
}

function log_app() {
  docker service logs dish_app "$@"
}

function log_services() {
  docker node ps
}

function log_service() {
  docker service logs -f "$1_$2"
}

function restart_ci() {
  # buildkite after a plain restart loses network, this fixes it
  docker-compose -f docker-internal.yml stop ci
  docker-compose -f docker-internal.yml rm -f ci
  docker-compose -f docker-internal.yml up -d ci
}

function nodes() {
  docker node ls
  docker node ps
}

function services() {
  docker service ls
  docker node ps
}

function setup_server() {
  host=$1
  echo "setting up $host"
  setup_server_ssh "$host"
  setup_server_docker "$host"
  setup_server_pre_sync "$host"
  sync_to "$host"
  setup_server_services "$host"
}

function setup_ci_server() {
  # resize /var partition first!
  # pvs
  # resize2fs /dev/mapper/vg00-var
  # lvextend -L +900G /dev/mapper/vg00-var
  local host=$1
  echo "setting up $host"
  setup_server_ssh "$host"
  setup_server_docker "$host"
  setup_server_pre_sync "$host"
  sync_to "$host"
  hostvar="$1_HOST"
  host="${!hostvar}"
  key="$PROJECT_ROOT/etc/keys/server_rsa"
  ssh -i "$key" -o StrictHostKeyChecking=no "root@$host" "
    echo setup post
    set -e
    unattended-upgrade -d
    cd /app
    ./dsh setup_docker_daemon_conf
    ./dsh setup_domain_certs
    echo restarting docker
    service docker restart
    cd /app
    ./dsh setup_compose
    ./dsh setup_buildkite
    ./dsh docker_login || true
    ./dsh deploy_ci
    echo done ✅
  "
}

function setup_server_ssh() {
  local hostvar="$1_HOST"
  local host="${!hostvar}"
  ssh-copy-id -i etc/keys/server_rsa "root@$host"
}

function setup_server_docker() {
  local hostvar="$1_HOST"
  local host="${!hostvar}"
  echo "$1 $host via $hostvar"
  docker-machine create --driver generic --generic-ip-address "$host" \
    --generic-ssh-user root --generic-ssh-key etc/keys/server_rsa "$1" \
    || echo "already exists? continue"
}

function setup_server_pre_sync() {
  local hostvar="$1_HOST"
  local host="${!hostvar}"
  key="$PROJECT_ROOT/etc/keys/server_rsa"
  ssh -i "$key" -o StrictHostKeyChecking=no "root@$host" "
    echo setup pre
    set -e
    apt update
    apt install -y fail2ban
    mkdir -p /app
    mkdir -p /var/data/buildkite
    mkdir -p /var/data/buildkite-docker
    mkdir -p /var/data/docker
    mkdir -p /var/data/gorse
    mkdir -p /var/data/traefik
    mkdir -p /var/data/portainer
    mkdir -p /var/data/registry
    mkdir -p /var/data/registry-mirror
    mkdir -p /var/data/postgresdb/production
    mkdir -p /var/data/postgresdb/test
    mkdir -p /var/data/timescaledb/production
    mkdir -p /var/data/timescaledb/test
    echo 'alias dsh=/app/dsh' >> ~/.bashrc
    echo 'source /app/.env' >> ~/.bashrc
    echo 'source /app/.env.production' >> ~/.bashrc
  "
}

function setup_server_services() {
  local hostvar="$1_HOST"
  local host="${!hostvar}"
  key="$PROJECT_ROOT/etc/keys/server_rsa"
  ssh -i "$key" -o StrictHostKeyChecking=no "root@$host" "
    echo setup post
    set -e
    ./dsh setup_docker_daemon_conf
    ./dsh setup_domain_certs
    echo restarting docker
    service docker restart
    cd /app
    ./dsh setup_compose
    ./dsh setup_swarm $1
    ./dsh docker_login
    # pre push images to registry
    ./dsh compose_build_and_push_all
    echo done
  "
}

function setup_docker_daemon_conf() {
  echo "linking in daemon conf"
  ln -s /app/etc/docker/daemon.json /etc/docker/daemon.json || echo exists, ok
}

function setup_domain_certs() {
  echo "copying certs"
  mkdir -p /var/data/traefik
  mv /var/data/traefik/acme-production.json /var/data/traefik/acme-production-bak.json || true
  cp ./etc/domain-certificates.json /var/data/traefik/acme-production.json
  chmod 600 /var/data/traefik/acme-production.json
}

# from a cold boot this should set up everything
function setup_swarm() {
  local hostvar="$1_HOST"
  local host="${!hostvar}"
  if [ "$host" = "" ]; then
    echo "no host given"
    exit 1
  fi
  docker swarm init --advertise-addr "$host"
  docker network create --driver=overlay traefik-public
  NODE_ID=$(docker info -f '{{.Swarm.NodeID}}')
  docker node update --label-add portainer.portainer-data=true "$NODE_ID"
  docker node update --label-add traefik-public.traefik-public-certificates=true "$NODE_ID"
  # for adding workers/managers
  # docker swarm join --token $(docker swarm join-token manager -q) "$d1_HOST:2377"
  # docker swarm join --token $(docker swarm join-token worker -q) "$d1_HOST:2377"
}

function log_command {
  echo "$" "$@"
  eval $(printf '%q ' "$@") </dev/tty
}

function ci_shell() {
  docker exec -it app_ci_1 bash
}

function psql_postgres() {
  PGPASSWORD="$POSTGRES_PASSWORD" log_command psql -d "$POSTGRES_DB" -U "$POSTGRES_USER" -p "$POSTGRES_PORT" -h localhost
}

function psql_timescale() {
  PGPASSWORD="$TIMESCALE_PASSWORD" log_command psql -d "$TIMESCALE_DB" -U "$TIMESCALE_USER" -p "$TIMESCALE_PORT" -h localhost
}

function generate_random_port() {
  echo "2$((1000 + RANDOM % 8999))"
}

function _setup_s3() {
  apk add --no-cache -X http://dl-cdn.alpinelinux.org/alpine/edge/testing s3cmd
}

function worker_ssh() {
  fly_tunnel
  log_command ssh -o StrictHostKeyChecking=no -l "root" "dish-worker.internal" -- "$@"
}

function worker_exec() {
  set -e
  worker_ssh "$@"
}

function stop_all_crawls() {
  curl -X 'POST' https://worker.dishapp.com/clear -H 'queues: all'
}

# comma separated
function stop_crawl() {
  curl -X 'POST' https://worker.dishapp.com/clear -H "queues: $@"
}

# Note that crawlers are also run on cron schedules.
# For cities list see: crawlers/src/utils "CITY_LIST"

# example: ./dsh start_crawler Yelp
function start_crawler() {
  worker_exec "node /app/services/crawlers/_/$1/all.js"
}

# example: ./dsh start_crawler_for_city yelp  Tucson, Arizona
function start_crawler_for_city() {
  worker_exec "node /app/services/crawlers/dist/$1/all.js --city \"$2\""
}

function start_all_crawlers_for_city() {
  reset_hosts
  set -e
  start_crawler_for_city "doordash" "$1"
  start_crawler_for_city "google" "$1"
  start_crawler_for_city "grubhub" "$1"
  start_crawler_for_city "infatuated" "$1"
  start_crawler_for_city "tripadvisor" "$1"
  start_crawler_for_city "yelp" "$1"
  start_crawler_for_city "ubereats" "$1"
}

function start_all_crawlers() {
  set -e
  start_crawler "doordash"
  start_crawler "google"
  start_crawler "grubhub"
  start_crawler "infatuated"
  start_crawler "tripadvisor"
  start_crawler "yelp"
  start_crawler "ubereats"
}

function all_crawlers_for_cities() {
  #start_all_crawlers_for_city 'San Francisco, CA'
  #start_all_crawlers_for_city 'Los Angeles, CA'
  #start_all_crawlers_for_city 'San Jose, CA'
  start_all_crawlers_for_city 'Redwood City, CA'
  start_all_crawlers_for_city 'Fremont, CA'
  start_all_crawlers_for_city 'San Rafael, CA'
  #start_all_crawlers_for_city 'Chicago, Illinois'
  #start_all_crawlers_for_city 'Tuscon, Arizona'
  #start_all_crawlers_for_city 'Istanbul, Turkey'
}

function crawl_self() {
  echo "Running self crawler"
  worker_exec "node /app/services/crawlers/dist/self/all.js"
}

function crawl_self_by_query() {
  [ -z "$1" ] && exit 1
  query="SELECT id FROM restaurant $1"
  echo "Running self crawler with SQL: $query"
  worker_exec "RUN=1 QUERY=${query@Q} node /app/services/crawlers/dist/self/one.js"
}

function crawl_self_sf_limited_cuisine() {
  query="
    WHERE st_dwithin(
      location, st_makepoint(-122.42, 37.76), 0.2
    )
    AND (
      tag_names @> '\"mexican__taco\"'
      OR
      tag_names @> '\"vietnamese__pho\"'
    )
  "
  crawl_self_by_query "$query"
}

function redis_command() {
  kubectl exec \
    redis-master-0 -n redis -c redis \
    -- bash -c "echo ${1@Q} | redis-cli"
}

function redis_cli() {
  fly_tunnel
  redis-cli -h "$REDIS_HOST" -a "$REDIS_PASSWORD" -p "$REDIS_PORT" "$@"
}

function redis_cli_list_all() {
  redis_cli keys "\*"
}

function redis_flush_all() {
  redis_command 'FLUSHALL'
}

function migrate_umami() {
  echo "migrating umami"
  psql "$POSTGRES_URL" <services/umami/setup.sql
}

function migrate_timescale() {
  echo "Migrating timescale $TIMESCALE_PORT_INTERNAL"
  pushd "$PROJECT_ROOT/services/timescale"
  npm install &>/dev/null || true
  TIMESCALE_PORT_INTERNAL=$TIMESCALE_PORT_INTERNAL node ./scripts/migrate.js
  popd
}

function migrate() {
  # TODO we have timescale-migrate service, just need hasura-migrate service now
  migrate_hasura
  migrate_timescale
}

function hasura_admin() {
  (cd services/hasura && hasura console --endpoint "http://localhost:$HASURA_PORT" --admin-secret=$HASURA_GRAPHQL_ADMIN_SECRET)
}

function migrate_hasura() {
  echo "migrating hasura"
  if ! [ -x "$(command -v hasura)" ]; then
    curl -L https://github.com/hasura/graphql-engine/raw/stable/cli/get.sh | bash
  fi
  echo "migrating db $POSTGRES_DB"
  pushd "$PROJECT_ROOT/services/hasura"
  echo "hasura migrate $HASURA_ENDPOINT"
  hasura --skip-update-check \
    migrate apply \
    --endpoint "$HASURA_ENDPOINT" \
    --admin-secret "$HASURA_GRAPHQL_ADMIN_SECRET"
  echo "hasura metadata"
  hasura --skip-update-check \
    metadata apply \
    --endpoint "$HASURA_ENDPOINT" \
    --admin-secret "$HASURA_GRAPHQL_ADMIN_SECRET"
  popd
  echo "init functions"
  pushd "$PROJECT_ROOT/services/hasura"
  cat functions/*.sql |
    PGPASSWORD=$POSTGRES_PASSWORD psql \
      -p "$POSTGRES_PORT" \
      -h localhost \
      -U "${POSTGRES_USER:-postgres}" \
      -d "${POSTGRES_DB:-dish}" \
      --single-transaction
  popd
}

function dump_scrape_data_to_s3() {
  _run_on_cluster postgres:12-alpine && return 0
  set -e
  _setup_s3
  copy_out="copy (select
      created_at,
      source,
      id_from_source,
      restaurant_id,
      location,
      data
      from scrape
    )
    to stdout with csv"
  echo "Dumping scrape table to S3..."
  PGPASSWORD="$POSTGRES_PASSWORD" psql \
    -h "$POSTGRES_HOST" \
    -U postgres \
    -d "${POSTGRES_DB:-dish}" \
    -c "$copy_out" |
    s3 put - "$DISH_BACKUP_BUCKET/scrape.csv"
  echo "...scrape table dumped tpo S3."
}

function bull_delete_queue() {
  queue=$1
  redis_command "EVAL \"\
    local keys = redis.call('keys', ARGV[1]) \
    for i=1,#keys,5000 do \
      redis.call('del', unpack(keys, i, math.min(i+4999, #keys))) \
    end \
    return keys \
  \" 0 bull:$queue*"
}

function dish_app_generate_tags() {
  export HASURA_ENDPOINT=https://hasura.dishapp.com
  export IS_LIVE=1
  pushd $PROJECT_ROOT/app
  node -r esbuild-register ./etc/generate_tags.ts
}

function s3() {
  s3cmd \
    --host sfo2.digitaloceanspaces.com \
    --host-bucket '%(bucket).sfo2.digitaloceanspaces.com' \
    --access_key "$DO_SPACES_ID" \
    --secret_key "$DO_SPACES_SECRET" \
    --human-readable-sizes \
    "$@"
}

function list_backups() {
  s3 ls "$DISH_BACKUP_BUCKET" | sort -k1,2
}

function backup_main_db() {
  set -e
  echo "backing up main db..."
  DUMP_FILE_NAME="dish-$DISH_ENV-db-backup-$(date +%Y-%m-%d-%H-%M).dump"
  pg_dump "$POSTGRES_URL" \
    -C -w --format=c |
    s3 put - "s3://dish-backups/$DUMP_FILE_NAME"
  echo 'Successfully backed up main database'
}

function backup_scrape_db() {
  set -e
  echo "backing up scrape db..."
  DUMP_FILE_NAME="dish-$DISH_ENV-scrape-backup-$(date +%Y-%m-%d-%H-%M).dump"
  pg_dump "$TIMESCALE_URL" -C -w --format=c |
    s3 put - "s3://dish-backups/$DUMP_FILE_NAME"
  echo 'Successfully backed up scrape database'
}

get_all_backups() {
  s3 ls "$DISH_BACKUP_BUCKET"
}

get_latest_main_backup() {
  echo $(
    s3 ls $DISH_BACKUP_BUCKET | grep 'dish-db' | tail -1 | awk '{ print $4 }'
  )
}

restore_latest_main_backup_to_local() {
  latest_backup=$(get_latest_main_backup)
  dump_file="/tmp/latest_dish_backup.dump"
  s3 get "$latest_backup" "$dump_file"
  # POSTGRES_PASSWORD=postgres pg_restore -h localhost -U postgres -p 5432 -d dish "$dump_file"
}

restore_latest_scrapes_backup_to_local() {
  latest_backup=$(get_latest_scrape_backup)
  dump_file="/tmp/latest_scrape_backup.dump"
  s3 get "$latest_backup" "$dump_file"
  POSTGRES_PASSWORD=postgres pg_restore -h localhost -U postgres -p 5433 -d dish "$dump_file"
}

get_latest_scrape_backup() {
  echo $(
    s3 ls $DISH_BACKUP_BUCKET |
      grep "dish-scrape-backup" |
      tail -1 |
      awk '{ print $4 }'
  )
}

function _restore_main_backup() {
  backup=$1
  echo "Restoring $backup ..."
  s3 get $backup backup.dump
  cat backup.dump | POSTGRES_PASSWORD=$POSTGRES_PASSWORD pg_restore \
    -h $POSTGRES_HOST \
    -U postgres \
    -d dish
}

function __restore_latest_main_backup() {
  _run_on_cluster postgres:12-alpine && return 0
  set -e
  _setup_s3
  _restore_main_backup "$(get_latest_main_backup)"
}

function restore_main_backup() {
  _run_on_cluster postgres:12-alpine && return 0
  set -e
  _setup_s3
  _restore_main_backup "$1"
}

function restore_latest_main_backup() {
  __restore_latest_main_backup
  migrate_hasura
}

function restore_latest_scrape_backup() {
  _run_on_cluster postgres:12-alpine && return 0
  set -e
  _setup_s3
  latest_backup=$(get_latest_scrape_backup)
  s3 get \$latest_backup backup.dump
  echo "Restoring \$latest_backup ..."
  cat backup.dump | POSTGRES_PASSWORD=$POSTGRES_PASSWORD pg_restore \
    -h $TIMESCALE_HOST \
    -U postgres \
    -d dish
}

function delete_unattached_volumes() {
  unattached_volumes="$(
    doctl compute volume list \
      --format ID,DropletIDs |
      rg -v '\[' |
      tr -s ' ' |
      cut -d ' ' -f 1 |
      tail -n +2
  )"
  echo "$unattached_volumes" | while read volume_id; do
    echo "Deleting $volume_id"
    doctl compute volume delete --force $volume_id
  done
}

function console() {
  ssh "root@$1.internal"
}

function find_app() {
  find "$PROJECT_ROOT" -type d \( -name node_modules -o -name packages -o -name dist -o -name _ -o -name src \) -prune -false -o -type d -name "$1" | head -n 1
}

function bull_clear() {
  curl -X POST https://dish-worker.fly.dev/clear
}

function bull_repl() {
  if [ "$1" = "" ]; then
    echo "Must specify a queue"
    echo "  its the constructor name of any class that extends WorkerJob"
    echo "  for example (at time of writing this)"
    echo "    Yelp, UberEats, GoogleReviewAPI, GooglePuppeteer, GoogleImages..."
    exit 0
  fi
  fly_tunnel
  "$PROJECT_ROOT/node_modules/.bin/bull-repl" connect \
    --host dish-redis.fly.dev \
    --port 10000 \
    --password redis \
    "$1"
}

function gorse_status() {
  _run_on_cluster alpine && return 0
  apk add --no-cache curl
  curl http://gorse:9000/status
}

function ping_home_page() {
  curl 'https://search.dishapp.com/top_cuisines?lon=-122.421351&lat=37.759251&distance=0.16'
}

function hasura_clean_event_logs() {
  main_db_command '
    DELETE FROM hdb_catalog.event_invocation_logs;
    DELETE FROM hdb_catalog.event_log
      WHERE delivered = true OR error = true;
  '
}

function scrapes_update_distinct_sources() {
  timescale_command '
    INSERT INTO distinct_sources(
      scrape_id, source, id_from_source
    ) SELECT DISTINCT ON (
      source, id_from_source
    ) id, source, id_from_source
    FROM scrape
  '
}

function crawler_mem_usage() {
  ps -eo size,pid,usconcer,command --sort -size |
    awk '{ hr=$1/1024 ; printf("%13.2f Mb ",hr) } { for ( x=4 ; x<=NF ; x++ ) { printf("%s ",$x) } print "" }' |
    cut -d "" -f2 |
    cut -d "-" -f1 |
    grep sandbox
}

function grafana_backup() {
  _run_on_cluster $DISH_REGISTRY/grafana-backup-tool && return 0
  set -e
  export GRAFANA_TOKEN="$GRAFANA_API_KEY"
  export GRAFANA_URL=https://grafana.k8s.dishapp.com
  grafana-backup save
  backup=$(ls _OUTPUT_/)
  destination=$DISH_BACKUP_BUCKET/grafan-backups
  s3 put _OUTPUT_/$backup $destination/$backup
  s3 ls $destination/ | sort -k1,2
}

function clean_dangling() {
  # remove dangling images which can mess up pull/push
  docker rmi $(docker images --filter "dangling=true" -q --no-trunc) || true
}

function docker_compose_push_core() {
  echo "pushing images..."
  docker push registry.dishapp.com/dish-base | sed -e 's/^/base: /;' &
  docker push registry.dishapp.com/dish-app | sed -e 's/^/app: /;' &
  docker push registry.dishapp.com/dish-hooks | sed -e 's/^/hooks: /;' &
  docker push registry.dishapp.com/dish-site | sed -e 's/^/site: /;' &
  docker push registry.dishapp.com/dish-worker | sed -e 's/^/worker: /;' &
  docker push registry.dishapp.com/dish-worker-proxy | sed -e 's/^/worker-proxy: /;' &
  docker push registry.dishapp.com/dish-run-tests | sed -e 's/^/run-tests: /;' &
  wait
}

function setup_buildkite() {
  if ! [ -x $(command -v buildkite-agent) ]; then
    echo "deb https://apt.buildkite.com/buildkite-agent stable main" | sudo tee /etc/apt/sources.list.d/buildkite-agent.list
    sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 32A37959C2FA5C3C99EFBC32A79206696452D198
    apt-get update && sudo apt-get install -y buildkite-agent
  fi
}

function setup_compose() {
  if ! [ -x "$(command -v docker-compose)" ]; then
    curl -L "https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
    chmod +x /usr/local/bin/docker-compose
  fi
}

function sync_to() {
  hostvar="$1_HOST"
  host="${!hostvar}"
  key="$PROJECT_ROOT/etc/keys/server_rsa"
  if [ "$host" = "" ] || [ ! -f "$key" ]; then
    echo "no host or private key $host $key"
    exit 1
  fi
  chmod 600 "$key"
  echo " ⬆️  syncing . to $host:/app"
  rsync \
    -avP -q --force \
    --exclude-from="$(git -C . ls-files --exclude-standard -oi --directory > /tmp/excludes; echo /tmp/excludes)" \
    --exclude='- .git' \
    -e "ssh -o StrictHostKeyChecking=no -i $key" . "root@$host:/app"
  echo "synced"
}
export -f sync_to

function sync_to_watch() {
  sync_to "$1"
  fswatch -0 -o path . | xargs -0 -n1 -I{} sh -c "sync_to $1"
}

function deploy_done_notify() {
  echo "notifying slack..."
  # post to slack
  commit=$(git rev-parse HEAD)
  # link="https://github.com/getdish/dish/tree/$commit"
  message="
  Successful deploy of $commit \n
  Code: https://github.com/getdish/dish/tree/$commit \n
  CI Run: $BUILDKITE_BUILD_URL
  "
  ./dsh send_slack_monitoring_message "$message"
}

function deploy_fail() {
  echo "Error: deploy failed due to exit code 😭😭😭"
  exit 1
}

function clean_docker_if_disk_full() {
  echo "checking if disk near full..."
  df -H | grep /dev/md2 | head -n 1 | awk '{ print $5 " " $1 }' | while read output; do
    used=$(echo "$output" | awk '{ print $1}' | cut -d'%' -f1)
    echo "$output used $used"
    if [ "$used" -ge 90 ]; then
      echo "running out of space, pruning..."
      if [ "$CLEAN_BUILDKITE_BUILDS" == "true" ]; then
        rm -r "/var/lib/buildkite/builds/*" || true
      fi
      docker image prune --all --filter "until=2h" --force || true
      docker system prune --filter "until=2h" --force || true
      docker volume rm $(docker volume ls -qf dangling=true)
      break
    fi
  done
  df -H | grep /dev/md2 | awk '{ print $5 " " $1 }' | while read output; do
    used=$(echo "$output" | awk '{ print $1}' | cut -d'%' -f1)
    if [ "$used" -ge 90 ]; then
      echo "really full, delete all.."
      docker system prune
      docker image prune --all
      rm -r /var/lib/buildkite/builds
      break
    fi
  done
}

function run_all_tests() {
  docker run \
    --net host \
    -v /var/run/docker.sock:/var/run/docker.sock \
    -v "$POSTGRES_DB_DIR:$POSTGRES_DB_DIR" \
    -v "$TIMESCALE_DB_DIR:$TIMESCALE_DB_DIR" \
    registry.dishapp.com/dish-run-tests:latest yarn test || (echo "failed tests" && exit 1)
}

function run_all_tests_in_compose() {
  exec timescale-migrate yarn test || echo "tests failed" && exit 1
}

function run_integration_tests() {
  echo "Running Test Cafe end-to-end browser-based tests..."
  pushd app
    docker run -d --net=host --name app-integration-tests-$BUILDKITE_BUILD_NUMBER $DISH_REGISTRY/app
  sleep 5
  ./test/testcafe.sh
  popd
}

function run_dsh() {
  esbuild ./dsh.ts  --target=node15 --format=esm --outdir=node_modules/.cache --out-extension:.js=.mjs && node ./node_modules/.bin/zx ./node_modules/.cache/dsh.mjs
}

# https://gist.github.com/mortensteenrasmussen/512f0566dbc3ef1cc4a2c47dd9cdb973
# function clean_registry() {
#   REGISTRY_DIR=YOUR_REGISTRY_DIR/data/docker/registry/v2/repositories
#   REGISTRY_URL=http://10.10.10.10:5000
#   #add --insecure to the curl command on line 17 if you use https with self-signed certificates

#   cd ${REGISTRY_DIR}
#   count=0

#   manifests_without_tags=$(comm -23 <(find . -type f -name "link" | grep "_manifests/revisions/sha256" | grep -v "\/signatures\/sha256\/" | awk -F/ '{print $(NF-1)}' | sort) <(for f in $(find . -type f -name "link" | grep "_manifests/tags/.*/current/link"); do cat ${f} | sed 's/^sha256://g'; echo; done | sort))

#   total_count=$(echo ${manifests_without_tags} | wc -w)

#   for manifest in ${manifests_without_tags}; do
#     repo=$(find . | grep "_manifests/revisions/sha256/${manifest}/link" | awk -F "_manifest"  '{print $(NF-1)}' | sed 's#^./\(.*\)/#\1#')

#     #should have error checking on the curl command, it might fail silently atm.
#     curl -s -X DELETE ${REGISTRY_URL}/v2/${repo}/manifests/sha256:${manifest} > /dev/null

#     ((count++))
#     echo "Deleted ${count} of ${total_count} manifests."
#   done
# }

function wait_until_hasura_ready() {
  echo "Waiting for Hasura to start ($HASURA_ENDPOINT)..."
  until [ $(curl -L $HASURA_ENDPOINT/healthz -o /dev/null -w '%{http_code}\n' -s) == "200" ];
    do sleep 0.1
  done
  echo "Hasura is up"
}
export -f wait_until_hasura_ready

function wait_until_postgres_ready() {
  echo "Waiting for Postgres to start ($POSTGRES_ENDPOINT)..."
  until [ $(curl -L $POSTGRES_ENDPOINT -o /dev/null -w '%{http_code}\n' -s) == "000" ];
    do sleep 0.1;
  done
  echo "Postgres is up"
}
export -f wait_until_postgres_ready

function wait_until_timescale_ready() {
  echo "Waiting for Timescale to start ($TIMESCALE_ENDPOINT)..."
  until [ $(curl -L $TIMESCALE_ENDPOINT -o /dev/null -w '%{http_code}\n' -s) == "000" ];
    do sleep 0.1;
  done
  echo "Timescale is up"
}
export -f wait_until_timescale_ready

function wait_until_dish_app_ready() {
  echo "Waiting for dish to start ($DISH_ENDPOINT)..."
  until [ $(curl -L $DISH_ENDPOINT/healthz -o /dev/null -w '%{http_code}\n' -s) == "200" ];
    do sleep 0.1;
  done
  echo "dish is up"
}
export -f wait_until_dish_app_ready

function wait_until_services_ready() {
  echo "Waiting for hasura to finish starting"
  if ! timeout --preserve-status 15 bash -c wait_until_hasura_ready; then
    echo "Timed out waiting for Hasura container to start"
    exit 1
  fi
  echo "Waiting for timescale to finish starting"
  if ! timeout --preserve-status 15 bash -c wait_until_timescale_ready; then
    echo "Timed out waiting for Timescale container to start"
    exit 1
  fi
  echo "TODO re-enable once certs fixed"
  # echo "Waiting for app to finish starting"
  # if ! timeout --preserve-status 15 bash -c wait_until_dish_app_ready; then
  #   echo "Timed out waiting for dish container to start"
  #   exit 1
  # fi
}

function clean_build() {
  find $PROJECT_ROOT -type d \( -name node_modules \) -prune -false -o -name "_" -type d -prune -exec rm -rf '{}' \; &
  find $PROJECT_ROOT -type d \( -name node_modules \) -prune -false -o -name "dist" -type d -prune -exec rm -rf '{}' \; &
  find $PROJECT_ROOT -type d \( -name node_modules \) -prune -false -o -name "tsconfig.tsbuildinfo" -prune -exec rm -rf '{}' \; &
  find $PROJECT_ROOT -type d \( -name node_modules \) -prune -false -o -name ".ultra.cache.json" -prune -exec rm -rf '{}' \; &
  wait
}

function clean() {
  clean_build
  find $PROJECT_ROOT -name "node_modules" -type d -prune -exec rm -rf '{}' \;
  find $PROJECT_ROOT -name "yarn-error.log" -prune -exec rm -rf '{}' \;
}

function run() {
  echo "executing: $ORIGINAL_ARGS in $CWD_DIR"
  pushd "$CWD_DIR"
  bash -c "$ORIGINAL_ARGS"
  popd "$CWD_DIR"
}

function disk_speed() {
  dd if=/dev/zero of=/tmp/test2.img bs=512 count=100 oflag=dsync
}

function setup_test_services() {
  echo "setup test services for env $DISH_ENV"
  docker network create traefik-public || true

  # if not already mounted/setup, we need to start postgres once and restart it
  # this is because there are problems where postgres gets corrupted/weird while other services
  # try and access it during init, giving it time to init here and then run later
  if [ ! -d "$POSTGRES_DB_DIR" ]; then
    echo "doing double start first time since no postgres db dir: $POSTGRES_DB_DIR"
    mkdir -p "$POSTGRES_DB_DIR"
    # only for buildkite non-docker mode, brittle
    chown -R buildkite-agent:buildkite-agent "$POSTGRES_DB_DIR"
    docker-compose run -d postgres
    echo "sourcing local env"
    source .env.local
    wait_until_postgres_ready
  fi

  echo "stopping"
  docker-compose stop -t 4

  echo "running"
  source_env
  compose_up -d

  echo "sourcing local to check and migrate"
  source .env.local
  wait_until_services_ready
  
  # TODO migrate on start method
  migrate_all

  echo "done"
}

function docker_registry_gc() {
  rm -r /var/data/registry-mirror/docker/registry/v2/repositories/dish-*
  docker image prune --all --filter "until=2h" --force || true
  docker system prune --filter "until=2h" --force || true
  docker volume rm $(docker volume ls -qf dangling=true)
  docker system prune
  clean_dangling
  for id in $(docker ps -aqf "name=registry-proxy" | xargs); do
    docker exec -it "$id" bin/registry garbage-collect /etc/docker/registry/config.yml
  done
}

function send_slack_monitoring_message() {
  message=$1
  curl -X POST "$SLACK_MONITORING_HOOK" \
    -H 'Content-type: application/json' \
    --silent \
    --output /dev/null \
    --show-error \
    --data @- <<EOF
    {
      "text": "$message",
    }
EOF
}

function source_env() {
  source .env
  arch="$(uname -m)"
  if [ "${arch}" = "arm64" ]; then
    source .env.m1
  fi
  # source current env next, .env.production by default
  if [ -f "$ENV_FILE" ]; then
    source "$ENV_FILE"
  else
    echo "Not loading ENV from $ENV_FILE as it doesn't exist"
  fi
}

# export DOCKER_TAG_NAMESPACE=${branch//\//-}
# export BASE_IMAGE=$DISH_REGISTRY/base:$DOCKER_TAG_NAMESPACE
pushd $PROJECT_ROOT >/dev/null
source_env
popd >/dev/null

function_to_run=$1

echo "dsh -- env $DISH_ENV in $PROJECT_ROOT running $function_to_run"

if [ "$NO_PRINT_HELP" != "1" ] && [ "$function_to_run" = "" ]; then
  for f in $(declare -F | cut -d ' ' -f3); do
   echo "${f}"
  done
  exit 0
fi


shift
ORIGINAL_ARGS="$@"
$function_to_run "$@"

if [ "$KEEP_ALIVE" = "true" ]; then
  tail -f dsh
fi
