#!/bin/bash
set -eo pipefail
#Does this put our `set -e` into all the functions?
export SHELLOPTS

DISH_ENV="${DISH_ENV:-production}"
ENV_FILE=".env.$DISH_ENV"

function shell() {
  ssh "root@$DEDICATED_SERVER_HOST"
}

function internal() {
  docker-compose -f docker-internal.yml "$@"
}

function ps_internal() {
  docker-compose -f docker-internal.yml ps
}

function sh_ci() {
  docker exec -it app_ci_1 sh
}

function deploy() {
  docker stack deploy --prune -c docker-compose.yml dish
  # any better log of the pull?
  printf "\n\nlast system logs:\n\n"
  tail -n 10 /var/log/syslog
}

function deploy_traefik() {
  docker stack deploy --prune -c docker-traefik.yml traefik
}

function deploy_internal() {
  docker-compose -f docker-internal.yml build ci
  docker-compose -f docker-internal.yml up -d ci
}

function logs_ci() {
  docker logs app_ci_1
}

function logs_traefik() {
  docker service logs traefik_traefik
}

function logs_app() {
  docker service logs dish_app
}

function logs_services() {
  docker node ps
}

function logs_service() {
  docker service logs "dish_${@:-registry}.1"
}

function exec() {
  app="${@:-app}"
  docker exec -ti dish_$app.1.$(docker service ps dish_$app -q --no-trunc | head -n1) /bin/bash
}

function exec_traefik() {
  exec traefik_traefik
}

function restart_ci() {
  docker-compose -f docker-internal.yml stop ci
  docker-compose -f docker-internal.yml rm -f ci
  docker-compose -f docker-internal.yml up -d ci
}

function stacks() {
  docker stack ls
  docker stack ps dish
}

function stacks_traefik() {
  docker stack ps traefik
}

function nodes() {
  docker node ls
  docker node ps
}

function services() {
  docker service ls
  docker node ps
}

function deploy_traefik() {
  docker stack deploy -c docker-traefik.yml traefik
}

function docker_login() {
  docker login registry.dishapp.com -u "$TRAEFIK_USERNAME" -p "$TRAEFIK_PASSWORD_PLAIN"
}

function docker_registry_gc() {
  rm -r /var/data/registry/registry-mirror-data/docker/registry/v2/repositories/dish-*
  docker image prune --all --filter "until=2h" --force || true
  docker system prune --filter "until=2h" --force || true
  docker volume rm $(docker volume ls -qf dangling=true)
  docker system prune
  clean_dangling
  for id in $(docker ps -aqf "name=registry-proxy" | xargs); do
    docker exec -it "$id" bin/registry garbage-collect /etc/docker/registry/config.yml
  done
}

function log_command {
  echo "$" "$@"
  eval $(printf '%q ' "$@") </dev/tty
}

function ci_shell() {
  docker exec -it app_ci_1 bash
}

function rsync_d1() {
  PRIVATE_KEY="$PROJECT_ROOT/etc/keys/d1_reliablesite_dish"
  echo "syncing"
  rsync \
    -avP --delete --filter=':- .gitignore' --filter='- .git' \
    -e "ssh -o StrictHostKeyChecking=no -i $PRIVATE_KEY" \
    . "root@$DEDICATED_SERVER_HOST:/app" &>/dev/null
  echo "synced"
}
export -f rsync_d1

function rsync_d1_watch() {
  fswatch -0 -o path . | xargs -0 -n1 -I{} sh -c 'rsync_d1'
}

# function start_swarm() {
#   SWARM_TOKEN=$(docker swarm join-token -q worker)
#   SWARM_MASTER=$(docker info --format "{{.Swarm.NodeAddr}}")
#   export REGISTRY_MIRROR_WORKER="${SWARM_MASTER}:${REGISTRY_PORT}"

#   # remove old
#   docker node rm --force $(docker node ls --filter "name=worker1" -q)
#   docker rm --force $(docker ps -q --filter "name=worker1")

#   # start new
#   docker-compose -f docker-swarm.yml up -d

#   # attach to swarm
#   sleep 10
#   docker exec -it app_worker1_1 docker swarm join --token "$SWARM_TOKEN" "$SWARM_MASTER:2377"
#   docker exec -it app_worker1_1 \
#     apk add curl \
#     && curl -L https://fly.io/install.sh | sh \
#     && FLY_API_TOKEN="$FLY_API_TOKEN" /root/.fly/bin/flyctl auth docker

#   # to show if its running
#   sleep 2
#   docker node ls
# }

ogpsql=$(which psql)

function psql() {
  PGPASSWORD="$POSTGRES_PASSWORD" log_command "$ogpsql" -d "$POSTGRES_DB" -U "$POSTGRES_USER" -p "$POSTGRES_PORT" -h "$POSTGRES_HOST"
}

function psql_timescale() {
  PGPASSWORD="$TIMESCALE_PASSWORD" log_command "$ogpsql" -d "$TIMESCALE_DB" -U "$TIMESCALE_USER" -p "$TIMESCALE_PORT" -h "$TIMESCALE_HOST"
}

function generate_random_port() {
  echo "2$((1000 + RANDOM % 8999))"
}

function _setup_s3() {
  apk add --no-cache -X http://dl-cdn.alpinelinux.org/alpine/edge/testing s3cmd
}

function send_slack_monitoring_message() {
  message=$1
  curl -X POST "$SLACK_MONITORING_HOOK" \
    -H 'Content-type: application/json' \
    --silent \
    --output /dev/null \
    --show-error \
    --data @- <<EOF
    {
      "text": "$message",
    }
EOF
}

function reset_hosts() {
  egrep -v "internal" ~/.ssh/known_hosts >/tmp/known_hosts &&
    rm ~/.ssh/known_hosts &&
    mv /tmp/known_hosts ~/.ssh/known_hosts
}

function worker_ssh() {
  fly_tunnel
  log_command ssh -o StrictHostKeyChecking=no -l "root" "dish-worker.internal" -- "$@"
}

function worker_exec() {
  set -e
  worker_ssh "$@"
}

function stop_all_crawls() {
  curl -X 'POST' https://worker.dishapp.com/clear -H 'queues: all'
}

# comma separated
function stop_crawl() {
  curl -X 'POST' https://worker.dishapp.com/clear -H "queues: $@"
}

# Note that crawlers are also run on cron schedules.
# For cities list see: crawlers/src/utils "CITY_LIST"

# example: ./dsh start_crawler Yelp
function start_crawler() {
  worker_exec "node /app/services/crawlers/_/$1/all.js"
}

# example: ./dsh start_crawler_for_city yelp  Tucson, Arizona
function start_crawler_for_city() {
  worker_exec "node /app/services/crawlers/dist/$1/all.js --city \"$2\""
}

function start_all_crawlers_for_city() {
  reset_hosts
  set -e
  start_crawler_for_city "doordash" "$1"
  start_crawler_for_city "google" "$1"
  start_crawler_for_city "grubhub" "$1"
  start_crawler_for_city "infatuated" "$1"
  start_crawler_for_city "tripadvisor" "$1"
  start_crawler_for_city "yelp" "$1"
  start_crawler_for_city "ubereats" "$1"
}

function start_all_crawlers() {
  set -e
  start_crawler "doordash"
  start_crawler "google"
  start_crawler "grubhub"
  start_crawler "infatuated"
  start_crawler "tripadvisor"
  start_crawler "yelp"
  start_crawler "ubereats"
}

function all_crawlers_for_cities() {
  #start_all_crawlers_for_city 'San Francisco, CA'
  #start_all_crawlers_for_city 'Los Angeles, CA'
  #start_all_crawlers_for_city 'San Jose, CA'
  start_all_crawlers_for_city 'Redwood City, CA'
  start_all_crawlers_for_city 'Fremont, CA'
  start_all_crawlers_for_city 'San Rafael, CA'
  #start_all_crawlers_for_city 'Chicago, Illinois'
  #start_all_crawlers_for_city 'Tuscon, Arizona'
  #start_all_crawlers_for_city 'Istanbul, Turkey'
}

function crawl_self() {
  echo "Running self crawler"
  worker_exec "node /app/services/crawlers/dist/self/all.js"
}

function crawl_self_by_query() {
  [ -z "$1" ] && exit 1
  query="SELECT id FROM restaurant $1"
  echo "Running self crawler with SQL: $query"
  worker_exec "RUN=1 QUERY=${query@Q} node /app/services/crawlers/dist/self/one.js"
}

function crawl_self_sf_limited_cuisine() {
  query="
    WHERE st_dwithin(
      location, st_makepoint(-122.42, 37.76), 0.2
    )
    AND (
      tag_names @> '\"mexican__taco\"'
      OR
      tag_names @> '\"vietnamese__pho\"'
    )
  "
  crawl_self_by_query "$query"
}

function redis_command() {
  kubectl exec \
    redis-master-0 -n redis -c redis \
    -- bash -c "echo ${1@Q} | redis-cli"
}

function redis_cli() {
  fly_tunnel
  redis-cli -h "$REDIS_HOST" -a "$REDIS_PASSWORD" -p 10000 "$@"
}

function redis_cli_list_all() {
  redis_cli keys "\*"
}

function redis_flush_all() {
  redis_command 'FLUSHALL'
}

function umami_migrate() {
  $ogpsql "$POSTGRES_URL" <services/umami/setup.sql
}

function db_migrate() {
  if ! [ -x "$(command -v hasura)" ]; then
    curl -L https://github.com/hasura/graphql-engine/raw/stable/cli/get.sh | bash
  fi
  echo "migrating db $POSTGRES_DB"
  pushd "$PROJECT_ROOT/services/hasura"
  echo "hasura migrate $HASURA_ENDPOINT"
  hasura --skip-update-check \
    migrate apply \
    --endpoint "$HASURA_ENDPOINT" \
    --admin-secret "$HASURA_GRAPHQL_ADMIN_SECRET"
  echo "hasura metadata"
  hasura --skip-update-check \
    metadata apply \
    --endpoint "$HASURA_ENDPOINT" \
    --admin-secret "$HASURA_GRAPHQL_ADMIN_SECRET"
  popd
}

function timescale_migrate() {
  echo "Migrating timescale"
  pushd "$PROJECT_ROOT/services/timescale"
  npm install &>/dev/null || true
  node ./scripts/migrate.js
  popd
}

function migrate() {
  hasura_migrate
  timescale_migrate
}

function hasura_admin() {
  (cd services/hasura && hasura console --endpoint "http://localhost:$HASURA_PORT" --admin-secret=$HASURA_GRAPHQL_ADMIN_SECRET)
}

function hasura_migrate() {
  echo "migrating hasura"
  db_migrate
  echo "init functions"
  pushd "$PROJECT_ROOT/services/hasura"
  cat functions/*.sql |
    PGPASSWORD=$POSTGRES_PASSWORD $ogpsql \
      -p "$POSTGRES_PORT" \
      -h localhost \
      -U "${POSTGRES_USER:-postgres}" \
      -d "${POSTGRES_DB:-dish}" \
      --single-transaction
  popd
}

function dump_scrape_data_to_s3() {
  _run_on_cluster postgres:12-alpine && return 0
  set -e
  _setup_s3
  copy_out="copy (select
      created_at,
      source,
      id_from_source,
      restaurant_id,
      location,
      data
      from scrape
    )
    to stdout with csv"
  echo "Dumping scrape table to S3..."
  PGPASSWORD="$POSTGRES_PASSWORD" $ogpsql \
    -h "$POSTGRES_HOST" \
    -U postgres \
    -d "${POSTGRES_DB:-dish}" \
    -c "$copy_out" |
    s3 put - "$DISH_BACKUP_BUCKET/scrape.csv"
  echo "...scrape table dumped tpo S3."
}

function bull_delete_queue() {
  queue=$1
  redis_command "EVAL \"\
    local keys = redis.call('keys', ARGV[1]) \
    for i=1,#keys,5000 do \
      redis.call('del', unpack(keys, i, math.min(i+4999, #keys))) \
    end \
    return keys \
  \" 0 bull:$queue*"
}

function dish_app_generate_tags() {
  export HASURA_ENDPOINT=https://hasura.dishapp.com
  export IS_LIVE=1
  pushd $PROJECT_ROOT/app
  node -r esbuild-register ./etc/generate_tags.ts
}

function s3() {
  s3cmd \
    --host sfo2.digitaloceanspaces.com \
    --host-bucket '%(bucket).sfo2.digitaloceanspaces.com' \
    --access_key "$DO_SPACES_ID" \
    --secret_key "$DO_SPACES_SECRET" \
    --human-readable-sizes \
    "$@"
}

function list_backups() {
  s3 ls "$DISH_BACKUP_BUCKET" | sort -k1,2
}

function backup_main_db() {
  set -e
  echo "backing up main db..."
  DUMP_FILE_NAME="dish-$DISH_ENV-db-backup-$(date +%Y-%m-%d-%H-%M).dump"
  pg_dump "$POSTGRES_URL" \
    -C -w --format=c |
    s3 put - "s3://dish-backups/$DUMP_FILE_NAME"
  echo 'Successfully backed up main database'
}

function backup_scrape_db() {
  set -e
  echo "backing up scrape db..."
  DUMP_FILE_NAME="dish-$DISH_ENV-scrape-backup-$(date +%Y-%m-%d-%H-%M).dump"
  pg_dump "$TIMESCALE_URL" -C -w --format=c |
    s3 put - "s3://dish-backups/$DUMP_FILE_NAME"
  echo 'Successfully backed up scrape database'
}

get_all_backups() {
  s3 ls "$DISH_BACKUP_BUCKET"
}

get_latest_main_backup() {
  echo $(
    s3 ls $DISH_BACKUP_BUCKET | grep 'dish-db' | tail -1 | awk '{ print $4 }'
  )
}

restore_latest_main_backup_to_local() {
  latest_backup=$(get_latest_main_backup)
  dump_file="/tmp/latest_dish_backup.dump"
  s3 get "$latest_backup" "$dump_file"
  # POSTGRES_PASSWORD=postgres pg_restore -h localhost -U postgres -p 5432 -d dish "$dump_file"
}

restore_latest_scrapes_backup_to_local() {
  latest_backup=$(get_latest_scrape_backup)
  dump_file="/tmp/latest_scrape_backup.dump"
  s3 get "$latest_backup" "$dump_file"
  POSTGRES_PASSWORD=postgres pg_restore -h localhost -U postgres -p 5433 -d dish "$dump_file"
}

get_latest_scrape_backup() {
  echo $(
    s3 ls $DISH_BACKUP_BUCKET |
      grep "dish-scrape-backup" |
      tail -1 |
      awk '{ print $4 }'
  )
}

function _restore_main_backup() {
  backup=$1
  echo "Restoring $backup ..."
  s3 get $backup backup.dump
  cat backup.dump | POSTGRES_PASSWORD=$POSTGRES_PASSWORD pg_restore \
    -h $POSTGRES_HOST \
    -U postgres \
    -d dish
}

function __restore_latest_main_backup() {
  _run_on_cluster postgres:12-alpine && return 0
  set -e
  _setup_s3
  _restore_main_backup "$(get_latest_main_backup)"
}

function restore_main_backup() {
  _run_on_cluster postgres:12-alpine && return 0
  set -e
  _setup_s3
  _restore_main_backup "$1"
}

function restore_latest_main_backup() {
  __restore_latest_main_backup
  db_migrate
}

function restore_latest_scrape_backup() {
  _run_on_cluster postgres:12-alpine && return 0
  set -e
  _setup_s3
  latest_backup=$(get_latest_scrape_backup)
  s3 get \$latest_backup backup.dump
  echo "Restoring \$latest_backup ..."
  cat backup.dump | POSTGRES_PASSWORD=$POSTGRES_PASSWORD pg_restore \
    -h $TIMESCALE_HOST \
    -U postgres \
    -d dish
}

function delete_unattached_volumes() {
  unattached_volumes="$(
    doctl compute volume list \
      --format ID,DropletIDs |
      rg -v '\[' |
      tr -s ' ' |
      cut -d ' ' -f 1 |
      tail -n +2
  )"
  echo "$unattached_volumes" | while read volume_id; do
    echo "Deleting $volume_id"
    doctl compute volume delete --force $volume_id
  done
}

function console() {
  ssh "root@$1.internal"
}

function find_app() {
  find "$PROJECT_ROOT" -type d \( -name node_modules -o -name packages -o -name dist -o -name _ -o -name src \) -prune -false -o -type d -name "$1" | head -n 1
}

function bull_clear() {
  curl -X POST https://dish-worker.fly.dev/clear
}

function bull_repl() {
  if [ "$1" = "" ]; then
    echo "Must specify a queue"
    echo "  its the constructor name of any class that extends WorkerJob"
    echo "  for example (at time of writing this)"
    echo "    Yelp, UberEats, GoogleReviewAPI, GooglePuppeteer, GoogleImages..."
    exit 0
  fi
  fly_tunnel
  "$PROJECT_ROOT/node_modules/.bin/bull-repl" connect \
    --host dish-redis.fly.dev \
    --port 10000 \
    --password redis \
    "$1"
}

function gorse_status() {
  _run_on_cluster alpine && return 0
  apk add --no-cache curl
  curl http://gorse:9000/status
}

function ping_home_page() {
  curl 'https://search.dishapp.com/top_cuisines?lon=-122.421351&lat=37.759251&distance=0.16'
}

function hasura_clean_event_logs() {
  main_db_command '
    DELETE FROM hdb_catalog.event_invocation_logs;
    DELETE FROM hdb_catalog.event_log
      WHERE delivered = true OR error = true;
  '
}

function scrapes_update_distinct_sources() {
  timescale_command '
    INSERT INTO distinct_sources(
      scrape_id, source, id_from_source
    ) SELECT DISTINCT ON (
      source, id_from_source
    ) id, source, id_from_source
    FROM scrape
  '
}

function crawler_mem_usage() {
  ps -eo size,pid,usconcer,command --sort -size |
    awk '{ hr=$1/1024 ; printf("%13.2f Mb ",hr) } { for ( x=4 ; x<=NF ; x++ ) { printf("%s ",$x) } print "" }' |
    cut -d "" -f2 |
    cut -d "-" -f1 |
    grep sandbox
}

function grafana_backup() {
  _run_on_cluster $DISH_REGISTRY/grafana-backup-tool && return 0
  set -e
  export GRAFANA_TOKEN="$GRAFANA_API_KEY"
  export GRAFANA_URL=https://grafana.k8s.dishapp.com
  grafana-backup save
  backup=$(ls _OUTPUT_/)
  destination=$DISH_BACKUP_BUCKET/grafan-backups
  s3 put _OUTPUT_/$backup $destination/$backup
  s3 ls $destination/ | sort -k1,2
}

function urlencode() {
  singles=${1//\'/\\\'}
  python -c "import sys, urllib.parse as ul; print(ul.quote('$singles'))"
}

function bert_sentiment() {
  text=$(urlencode "$1")
  url="https://bert.k8s.dishapp.com/?text=$text"
  curl "$url"
}

function docker_build() {
  docker-compose build "$@"
}

function docker_build_file() {
  docker buildx build --progress=plain --platform linux/amd64 "$@"
}

function clean_dangling() {
  # remove dangling images which can mess up pull/push
  docker rmi $(docker images --filter "dangling=true" -q --no-trunc) || true
}

function docker_compose_build() {
echo "ðŸ³ docker building... $@"
  if [ "$INTERNAL" = "true" ]; then
    docker-compose -f docker-internal.yml build "$@"
  else
    docker-compose build "$@" && docker-compose push "$@"
  fi
}

function docker_compose_build_and_push() {
  docker_compose_build "$@"
  echo "ðŸ³ docker pushing... $@"
  docker push "registry.dishapp.com/dish-$@"
}

function docker_compose_push_core() {
  docker push registry.fly.io/dish-base | sed -e 's/^/base: /;' &
  docker push registry.fly.io/dish-app | sed -e 's/^/app: /;' &
  docker push registry.fly.io/dish-hooks | sed -e 's/^/hooks: /;' &
  docker push registry.fly.io/dish-site | sed -e 's/^/site: /;' &
  docker push registry.fly.io/dish-worker | sed -e 's/^/worker: /;' &
}

function docker_compose_up_subset() {
  services=$1
  extra=$2
  flags=""
  echo "docker-compose up $flags $extra $services; ($POSTGRES_DB)"
  if [ "$DOCKER_NO_RECREATE" != "true" ]; then
    flags="--remove-orphans --force-recreate"
  fi
  if [ -z "$extra" ]; then
    docker-compose up $flags $services
  else
    docker-compose up $flags "$extra" $services
  fi
  printf "\n\n\n"
}

function docker_compose_up() {
  services_list="$COMPOSE_EXCLUDE${COMPOSE_EXCLUDE_EXTRA:-}"
  services=$(
    docker-compose config --services 2>/dev/null |
      grep -E -v "$services_list" |
      tr '\r\n' ' '
  )
  docker_login
  # cleans up misbehaving old containers
  if [ "$DISH_ENV" = "test" ]; then
    for service in $services; do
      docker-compose rm --force "$service" || true
    done
  fi
  docker_compose_up_subset "$services" "$@"
}

# bypass filtering and stuff
function up() {
  docker-compose up "$@"
}

function deploy_dedicated() {
  source .env.d1
  SERVICES=$(
    docker-compose config --services 2> /dev/null \
      | grep -E -v "$COMPOSE_EXCLUDE" \
      | tr '\r\n' ' '
  )
  echo "ðŸ–¥ deploying dedicated server: $SERVICES (syncing)..."
  rsync \
    -avP --delete --filter=':- .gitignore' --filter='- .git' \
    -e "ssh -o StrictHostKeyChecking=no -i $PRIVATE_KEY" \
    . "root@$DEDICATED_SERVER_HOST:/app" &> /dev/null
  echo " running commands..."
  ssh \
    -i "$PRIVATE_KEY" \
    -o StrictHostKeyChecking=no \
    "root@$DEDICATED_SERVER_HOST" "
      set -e
      cd /app
      source .env
      source .env.production
      source .env.d1
      yarn
      flyctl auth docker
      dsh deploy
      echo todo migrate on start
      sleep 10
      dsh hasura_migrate
      dsh timescale_migrate
      dsh umami_migrate
      docker system prune --force &
      dsh wait_until_services_ready &
      wait
      echo done
    "
  printf "\n\n ðŸ–¥ deploying dedicated server done âœ… \n\n"
}

function deploy_done_notify() {
  # post to slack
  commit=$(git rev-parse HEAD)
  # link="https://github.com/getdish/dish/tree/$commit"
  message="
  Successful deploy of $commit \n
  Code: https://github.com/getdish/dish/tree/$commit \n
  CI Run: $BUILDKITE_BUILD_URL
  "
  ./dsh send_slack_monitoring_message "$message"
}

function build() {
  docker-compose build "$@"
}

function deploy_fail() {
  echo "Error: deploy failed due to exit code ðŸ˜­ðŸ˜­ðŸ˜­"
  exit 1
}

function clean_docker_if_disk_full() {
  echo "checking if disk near full..."
  df -H | grep /dev/nvme0n1p7 | head -n 1 | awk '{ print $5 " " $1 }' | while read output; do
    used=$(echo "$output" | awk '{ print $1}' | cut -d'%' -f1)
    echo "$output used $used"
    if [ "$used" -ge 90 ]; then
      echo "running out of space, pruning..."
      if [ "$CLEAN_BUILDKITE_BUILDS" == "true" ]; then
        rm -r "/var/lib/buildkite/builds/*" || true
      fi
      docker image prune --all --filter "until=2h" --force || true
      docker system prune --filter "until=2h" --force || true
      docker volume rm $(docker volume ls -qf dangling=true)
      break
    fi
  done
  df -H | grep /dev/nvme0n1p7 | awk '{ print $5 " " $1 }' | while read output; do
    used=$(echo "$output" | awk '{ print $1}' | cut -d'%' -f1)
    if [ "$used" -ge 90 ]; then
      echo "really full, delete all.."
      docker system prune
      docker image prune --all
      rm -r /var/lib/buildkite/builds
      break
    fi
  done
}

function run_all_tests() {
  docker run \
    --net host \
    -v /var/run/docker.sock:/var/run/docker.sock \
    registry.dishapp.com/dish-run-tests:latest \
    yarn test
}

function run_integration_tests() {
  echo "Running Test Cafe end-to-end browser-based tests..."
  pushd app
    docker run -d --net=host --name app-integration-tests-$BUILDKITE_BUILD_NUMBER $DISH_REGISTRY/app
  sleep 5
  ./test/testcafe.sh
  popd
}

# https://gist.github.com/mortensteenrasmussen/512f0566dbc3ef1cc4a2c47dd9cdb973
# function clean_registry() {
#   REGISTRY_DIR=YOUR_REGISTRY_DIR/data/docker/registry/v2/repositories
#   REGISTRY_URL=http://10.10.10.10:5000
#   #add --insecure to the curl command on line 17 if you use https with self-signed certificates

#   cd ${REGISTRY_DIR}
#   count=0

#   manifests_without_tags=$(comm -23 <(find . -type f -name "link" | grep "_manifests/revisions/sha256" | grep -v "\/signatures\/sha256\/" | awk -F/ '{print $(NF-1)}' | sort) <(for f in $(find . -type f -name "link" | grep "_manifests/tags/.*/current/link"); do cat ${f} | sed 's/^sha256://g'; echo; done | sort))

#   total_count=$(echo ${manifests_without_tags} | wc -w)

#   for manifest in ${manifests_without_tags}; do
#     repo=$(find . | grep "_manifests/revisions/sha256/${manifest}/link" | awk -F "_manifest"  '{print $(NF-1)}' | sed 's#^./\(.*\)/#\1#')

#     #should have error checking on the curl command, it might fail silently atm.
#     curl -s -X DELETE ${REGISTRY_URL}/v2/${repo}/manifests/sha256:${manifest} > /dev/null

#     ((count++))
#     echo "Deleted ${count} of ${total_count} manifests."
#   done
# }

function is_hasura_up() {
  [ $(curl -L $HASURA_ENDPOINT/healthz -o /dev/null -w '%{http_code}\n' -s) == "200" ]
}
export -f is_hasura_up

function is_timescale_up() {
  [ $(curl -L $TIMESCALE_ENDPOINT -o /dev/null -w '%{http_code}\n' -s) == "000" ]
}
export -f is_timescale_up

function is_postgres_up() {
  [ $(curl -L $POSTGRES_ENDPOINT -o /dev/null -w '%{http_code}\n' -s) == "000" ]
}
export -f is_postgres_up

function is_dish_up() {
  [ $(curl -L $DISH_ENDPOINT/healthz -o /dev/null -w '%{http_code}\n' -s) == "200" ]
}
export -f is_dish_up

function wait_until_hasura_ready() {
  echo "Waiting for Hasura to start ($HASURA_ENDPOINT)..."
  until is_hasura_up; do sleep 0.1; done
  echo "Hasura is up"
}
export -f wait_until_hasura_ready

function wait_until_postgres_ready() {
  echo "Waiting for Postgres to start ($POSTGRES_ENDPOINT)..."
  until is_postgres_up; do sleep 0.1; done
  echo "Postgres is up"
}
export -f wait_until_postgres_ready

function wait_until_timescale_ready() {
  echo "Waiting for Timescale to start ($TIMESCALE_ENDPOINT)..."
  until is_timescale_up; do sleep 0.1; done
  echo "Timescale is up"
}
export -f wait_until_timescale_ready

function wait_until_dish_app_ready() {
  echo "Waiting for dish to start ($DISH_ENDPOINT)..."
  until is_dish_up; do sleep 0.1; done
  echo "dish is up"
}
export -f wait_until_dish_app_ready

function wait_until_services_ready() {
  echo "Waiting for hasura to finish starting"
  if ! timeout --preserve-status 25 bash -c wait_until_hasura_ready; then
    echo "Timed out waiting for Hasura container to start"
    exit 1
  fi
  echo "Waiting for timescale to finish starting"
  if ! timeout --preserve-status 25 bash -c wait_until_timescale_ready; then
    echo "Timed out waiting for Timescale container to start"
    exit 1
  fi
  echo "Waiting for app to finish starting"
  if ! timeout --preserve-status 25 bash -c wait_until_dish_app_ready; then
    echo "Timed out waiting for dish container to start"
    exit 1
  fi
}

function clean_build() {
  find $PROJECT_ROOT -type d \( -name node_modules \) -prune -false -o -name "_" -type d -prune -exec rm -rf '{}' \; &
  find $PROJECT_ROOT -type d \( -name node_modules \) -prune -false -o -name "dist" -type d -prune -exec rm -rf '{}' \; &
  find $PROJECT_ROOT -type d \( -name node_modules \) -prune -false -o -name "tsconfig.tsbuildinfo" -prune -exec rm -rf '{}' \; &
  find $PROJECT_ROOT -type d \( -name node_modules \) -prune -false -o -name ".ultra.cache.json" -prune -exec rm -rf '{}' \; &
  wait
}

function clean() {
  clean_build
  find $PROJECT_ROOT -name "node_modules" -type d -prune -exec rm -rf '{}' \;
  find $PROJECT_ROOT -name "yarn-error.log" -prune -exec rm -rf '{}' \;
}

function run() {
  bash -c "$ORIGINAL_ARGS"
}

function setup_test_services() {
  docker network create traefik-public || true

  # if not already mounted/setup, we need to start postgres once and restart it
  # this is because there are problems where postgres gets corrupted/weird while other services
  # try and access it during init, giving it time to init here and then run later
  if [ ! -d "$POSTGRES_DB_DIR" ]; then
    echo "doing double start first time since no postgres db dir: $POSTGRES_DB_DIR"
    mkdir -p "$POSTGRES_DB_DIR"
    docker-compose run -d postgres
    wait_until_postgres_ready
  fi

  # saw some hanging around, just in case run this its pretty fast
  echo "down"
  docker-compose down --remove-orphans -t 4

  # fixes issues of not allowing connecting during tests
  if [ -d "$POSTGRES_DB_DIR" ]; then
    conf="$POSTGRES_DB_DIR/pg_hba.conf"
    grep -qxF 'host all all all trust' "$conf" || echo 'host all all all trust' >> "$conf"
  fi
  if [ -d "$TIMESCALE_DB_DIR" ]; then
    conf="$TIMESCALE_DB_DIR/pg_hba.conf"
    grep -qxF 'host all all all trust' "$conf" || echo 'host all all all trust' >> "$conf"
  fi

  docker_compose_up -d
  wait_until_services_ready
  migrate

  echo "done"
}

if command -v git &>/dev/null; then
  export PROJECT_ROOT=$(git rev-parse --show-toplevel)
  # branch=$(git rev-parse --abbrev-ref HEAD)
  # export DOCKER_TAG_NAMESPACE=${branch//\//-}
  # export BASE_IMAGE=$DISH_REGISTRY/base:$DOCKER_TAG_NAMESPACE
  pushd $PROJECT_ROOT >/dev/null
  set -a
  source .env
  arch="$(uname -m)"
  if [ "${arch}" = "arm64" ]; then
    source .env.m1
  fi
  # source current env next, .env.production by default
  if [ -f "$ENV_FILE" ]; then
    source "$ENV_FILE"
  else
    echo "Not loading ENV from $ENV_FILE as it doesn't exist"
  fi
  set +a
  echo "dsh -- env $DISH_ENV $IS_M1"
  popd >/dev/null
else
  echo "Not loading ENV as there's no \`git\` command"
  export all_env="$(env)"
fi


function_to_run=$1


if [ "$NO_PRINT_HELP" != "1" ] && [ "$function_to_run" = "" ]; then
  for f in $(declare -F | cut -d ' ' -f3); do
   echo "${f}"
  done
  exit 0
fi


shift
ORIGINAL_ARGS="$@"
$function_to_run "$@"
